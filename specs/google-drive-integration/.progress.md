# Progress: google-drive-integration

## Original Goal

Google Drive API integration. I want to have the FERPA pipeline interact with a Google Drive folder, ingest documents from Google Drive folders that will be shared with the program, then perform all of the pipeline steps.

## Phase History

- [research] Started: 2026-01-16, Completed: 2026-01-16
- [requirements] Started: 2026-01-16, Completed: 2026-01-16
- [design] Started: 2026-01-17

## Completed Tasks

- Research phase completed with technology recommendations
- Requirements document created (requirements.md)
- Design document created (design.md)
- [x] 1.1 Create gdrive module structure and error types - 2b76383
- [x] 1.2 Implement OAuth2 authenticator for local development - aa4e40a
- [x] 1.3 Implement FolderDiscovery with FolderMap - 35724b6
- [x] 1.4 Quality Checkpoint - b3a5dca
- [x] 1.5 Implement DocumentDownloader with BytesIO streaming - 436741a
- [x] 1.6 Modify DocumentParser to accept BytesIO - 781e2f3
- [x] 1.7 Implement ResultUploader - afdf122
- [x] 1.8 Quality Checkpoint - 478e46d
- [x] 1.9 Implement DriveProcessor orchestrator - f1aac4b
- [x] 1.10 Add gdrive-process CLI command - 071d249
- [x] 1.11 Update settings.yaml with gdrive section - 6261304
- [x] 1.12 Update pyproject.toml with google-auth dependency - 8f662f8
- [x] 1.13 Quality Checkpoint - 0ca3418
- [x] 1.14 POC Checkpoint - End-to-end validation (pending manual testing with real Drive credentials)
- [x] 2.1 Add rate limiter for API calls - bb3660f

## Current Task

Task 2.2: Implement Workload Identity Federation authenticator

## Learnings

- Google API dependencies already in pyproject.toml: google-api-python-client>=2.100.0, google-auth-oauthlib>=1.1.0
- Service Account auth preferred for automated pipelines - share folder with SA email like a regular user
- Google strongly recommends Workload Identity Federation over service account keys for GCP deployments
- TeacherDocument.source_path already documented to accept "Original file path or Google Drive ID" - no model changes needed
- python-docx supports BytesIO streams - minimal changes to DocumentParser.parse_docx() for stream input
- Google Drive API quota: 12,000 queries per 60 seconds (generous for this use case)
- Google Docs export limited to 10MB - large documents need special handling
- Push notifications require HTTPS webhook + domain verification + 24-hour renewal - polling simpler for batch
- FERPA compliance maintained: documents downloaded via HTTPS, processed locally through existing FERPA gate
- Google Workspace for Education is FERPA-compliant when properly configured per institution
- Changes API can track processed files to avoid reprocessing - use getStartPageToken/list pattern
- [Requirements] Existing pipeline has 6 stages (0-5) with Stage 3 as FERPA gate - all integration must preserve this boundary
- [Requirements] DocumentParser._detect_format() supports COMBINED_HEADER, SEPARATE_HEADER, TABLE - Drive docs must export to compatible format
- [Requirements] Current CLI uses Typer with rich console - new gdrive-process command should follow same patterns
- [Requirements] Existing batch processing (pipeline.process_batch) handles multiple files with error isolation
- [Requirements] settings.yaml has checkpoint_dir for local checkpoints - GCS equivalent needed for serverless
- [Requirements] The anonymization verification provides is_clean boolean - must integrate with Drive flow
- [Requirements] RosterLoader only supports CSV - Google Sheets roster support is out of scope for now
- [Requirements] User confirmed "Editor" permission required for upload capability, not just "Viewer"
- [Requirements] 11 user stories identified covering auth, access, download, processing, upload, CLI, scheduling, folder discovery, sprint selection, progress tracking, and batch scale
- [Requirements] User clarified: NO hardcoded folder assumptions - dynamic discovery at EACH run, not cached
- [Requirements] Sprint/folder selection is critical - need to process "September Comments" across all teachers with one command
- [Requirements] Leaf folder = folder with documents but no subfolders = processing target
- [Design] DocumentParser.parse_docx() only change: accept Union[Path, BytesIO] as first argument - python-docx's DocxDocument() already handles both
- [Design] Glob pattern matching (fnmatch) chosen over regex for --target-folder because it's familiar syntax and case-insensitive
- [Design] FolderNode dataclass used instead of Pydantic for internal data structures (follows codebase pattern)
- [Design] Cloud Run chosen over Cloud Functions due to 60-minute processing window (Functions max 9 min)
- [Design] Thread pool (5 concurrent) chosen for parallel downloads to balance speed vs rate limits
- [Design] Existing FeedbackPipeline.process_document() can be reused directly - just need to pass BytesIO to DocumentParser
- [Design] Rate limiting via fixed window (900/100sec) to stay safely under Google's 1000/100sec quota
- [Design] Results uploaded to "pipeline_outputs" subfolder to keep source folders clean
- [Design] Position-based metadata extraction (depth 1=house, 2=teacher, 3=period) - simple and matches documented structure
- [Design] Error handling: continue-on-error for batch processing - one bad doc shouldn't stop 240-doc batch
- [Design] Workload Identity Federation requires: project_id, pool_id, provider_id, service_account_email
- [Task 1.1] Use typing.Optional and typing.List/Dict for Python 3.9 compatibility (not str | None syntax)
- [Task 1.4] Quality checkpoint: Fixed mypy strict mode errors (no-untyped-call for google auth libs, missing type params for Dict), ruff errors (unsorted imports, unused imports/variables)
- [Task 1.8] Quality checkpoint: Removed unused `type: ignore[import-untyped]` comments from uploader.py and downloader.py - these become unused when using --ignore-missing-imports flag
- [Task 1.13] Quality checkpoint: Fixed cli.py (removed unused TYPE_CHECKING import, simplified if-else to ternary, removed f-prefix from strings without placeholders) and stage_3_anonymize.py (removed duplicate "wade" in set, simplified bool return, renamed unused is_explicit to _is_explicit, combined nested if statements)

## User Decisions

- **Deployment**: GCP (FERPA-compliant, Workspace for Education agreement likely covers it)
- **Authentication**: Service Account with Workload Identity Federation
- **Folder structure**: 3-level hierarchy (House → Teacher → Period), 1 doc per period folder
- **Processing trigger**: Scheduled (Cloud Scheduler + Cloud Run/Functions)
- **Output location**: Upload reports back to Drive (requires Editor permission)

## Folder Structure (from user)

```
Root/
├── House (×10)/
│   └── Teacher (×6 per house)/
│       ├── September Comments/ → 1 doc
│       ├── Interim 1 Comments/ → 1 doc
│       ├── Interim 3 Comments/ → 1 doc
│       └── Sponsor Letters/    → 1 doc
```

Scale: 10 × 6 × 4 = 240 documents per cycle

## Key Requirement: Dynamic Folder Discovery (Updated)

User clarified: NO hardcoded folder structure assumptions. Instead:
- `discover_structure(root_folder_id)` runs at START of each ingestion
- Returns a folder map (JSON) with IDs, names, depths, relationships
- Identifies "leaf folders" (containing docs, no subfolders) as processing targets
- Zero assumptions about hierarchy levels - works with ANY structure

## Key Requirement: Sprint/Folder Selection

User wants to toggle which folder (sprint) to process:
- `--target-folder <name>` filters to specific folders (e.g., "September Comments")
- Pattern matching supported (e.g., "Interim*" matches "Interim 1", "Interim 3")
- `--list-folders` shows discovered structure without processing
- Default: process ALL leaf folders if no filter specified

## Blockers

- None currently

## Next

Task 2.2: Implement Workload Identity Federation authenticator

## POC Manual Testing Required

Phase 1 (POC) code is complete. Manual validation with real Google Drive credentials is required:
1. Set up OAuth2 credentials (client_secrets.json)
2. Share a test Drive folder with test documents
3. Run: `ferpa-feedback gdrive-process <folder-id> --list-folders`
4. Run: `ferpa-feedback gdrive-process <folder-id> --dry-run`
5. Run: `ferpa-feedback gdrive-process <folder-id> --output-local ./test_output`

## Task Planning Learnings

- Total tasks: 32 across 4 phases (POC: 14, Refactoring: 8, Testing: 7, Quality Gates: 7)
- Task dependencies identified: downloader/uploader depend on discovery for DriveDocument type; processor depends on all components
- Quality checkpoints inserted every 2-3 tasks (7 total checkpoints)
- POC shortcuts: OAuth2 only, sequential downloads, OVERWRITE mode only, no rate limiting - all addressed in Phase 2
- Risk areas: OAuth2 token refresh during long batches, Google Docs export formatting, rate limits for large hierarchies, BytesIO memory
- DocumentParser modification (task 1.6) is independent and can be done in parallel with auth/discovery
- Cloud deployment (Phase 4) includes Dockerfile, cloudbuild.yaml, and Cloud Run handler
- Deferred to future work: Changes API tracking, progress checkpointing, email notifications, Terraform IaC
- Test strategy: 6 test files covering discovery, downloader, uploader, ingestion, integration
- Existing google-api-python-client and google-auth-oauthlib already in pyproject.toml - only need google-auth and google-auth-httplib2
